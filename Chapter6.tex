\chapter{Automatic generation of labels for topics' bags of words}
\doublespacing
\label{chap:label}
\minitoc



\section{Introduction: finding labels to represent a topic}
In natural language processing and information retrieval, topic modeling classically uses bags of words to represent the meaning of a text. However, this is not sufficient to support user interactions as bags of words require an effort from the user to go through the lists of the most important words in order to get an idea of the topic these words represent when considered together.
In chapter \ref{chap:ttd} we discussed a method that extracts topics from tags and the outputs of topic model are indeed bags of words, each of them representing a detected topic of interest. At this stage we could only attach meaningless labels for each topic, such as \textit{topic 3}, \textit{topic 5}.

Let us now consider examples of such topics: 
\begin{itemize}
    \item {italy, florence, venice, tuscany -> \textit{Italy}}
    \item {git, svn, tfs, maven -> \textit{version-control}}
    \item {machine-learning, artificial-intelligence, neural-network, classification -> \textit{artificial-intelligence}}
\end{itemize}

The labels, (e.g. \textit{Italy}, \textit{version-control}, \textit{artificial-intelligence}), in the right hand side are good candidates to summarize the overall topics captured by the bags the words on the left hand side. Those labels are at least more informative than labels such as \textit{topic 3} and\textit{topic 5} and can be used in interfaces and graphical representations of the results of the detection of communities of interest.

So an interesting task we consider in this chapter is how to automatically generate a general label for the bags of words representing a topic, which can best represent the meaning of that topic. \cite{chp6OnConceptualLabelingOfBagOfWords} introduces the task of conceptual labelling (CL), which aims at generating a minimum set of conceptual labels that best summarize a bag of words. Our work is similar to this one, but the main difference is that we use DBpedia as external knowledge and we use graph centrality based algorithms to help generate labels to represent a bag of words. \cite{chp2hulpus2013unsupervisedtopiclabeling} also proposes to use DBpedia and graph centrality based algorithms to choose labels. Our approach differs from it in that rather than using existing graph centrality based algorithms to generate labels, it is a hybrid method. 


\subsection{Problem definition: words, topics and labels}
Our previous work on topic modeling can generate topics from words or tags. Each topic consists of several tags or words. Table \ref{tab:flickrtags} and table \ref{tab:stacktags} list some detected topics from a Flickr dataset and StackOverflow dataset. The topic extraction algorithms are able to put closely related words or tags into the same topic, however, they can only use meaningless IDs (e.g. topic 3) to represent a topic. Our goal in this chapter is to find a label (e.g. aviation) to replace the original label (e.g. topic 3).



\begin{table}[htp]
%\begin{table}[!t]
\caption{Top tags and their probabilities on the Flickr dataset}
\label{tab:flickrtags}
\centering
%\scriptsize
%\begin{tabular}{|p{43pt}|p{10pt}|p{37pt}|p{10pt}|p{50pt}|p{10pt}|}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{topic3} & \multicolumn{2}{c|}{topic4} & \multicolumn{2}{c|}{topic5}  \\
\hline
airplane&0.074&tshirt&0.216&music&0.077\\ \hline
airport&0.053&shirt&0.154&rock&0.040\\ \hline
aircraft&0.029&shirts&0.112&concert&0.036\\ \hline
flying&0.028&threadless&0.109&live&0.025\\ \hline
plane&0.027&tshirts&0.009&band&0.022\\ \hline
aviation&0.022&tee&0.008&singing&0.019\\ \hline
flight&0.014&clothing&0.007&guitar&0.018\\ \hline
aeroplane&0.012&media&0.006&festival&0.017\\ \hline
jet&0.010&models&0.006&show&0.014\\ \hline
boeing&0.009&camiseta&0.004&livemusic&0.010\\ \hline
\hline
\multicolumn{2}{|c|}{topic23} & \multicolumn{2}{c|}{topic24} & \multicolumn{2}{c|}{topic25}  \\
\hline
italy&0.179&bike&0.114&portrait&0.049\\ \hline
italia&0.053&motorcycle&0.052&girl&0.029\\ \hline
rome&0.028&racing&0.033&woman&0.014\\ \hline
florence&0.021&bicycle&0.028&smile&0.014\\ \hline
venice&0.014&race&0.027&model&0.010\\ \hline
tuscany&0.014&motorbike&0.024&sexy&0.009\\ \hline
roma&0.011&sport&0.019&face&0.008\\ \hline
europe&0.011&speedway&0.011&fun&0.008\\ \hline
firenze&0.010&500cc&0.010&man&0.008\\ \hline
milan&0.007&methanol&0.010&love&0.008\\ \hline
\end{tabular}
\end{table}



\begin{table}[htp]
%\begin{table}[!t]
\caption{Top tags and their probabilities on stackoverflow dataset}
\label{tab:stacktags}
\centering
%\scriptsize
%\begin{tabular}{|p{43pt}|p{10pt}|p{37pt}|p{10pt}|p{50pt}|p{10pt}|}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{topic4} & \multicolumn{2}{c|}{topic5} & \multicolumn{2}{c|}{topic6}  \\
\hline
iphone&0.203&git&0.198&sql&0.177\\
\hline
objective-c&0.112&svn&0.096&mysql&0.122\\
\hline
ios&0.109&version-control&0.045&sql-server&0.074\\
\hline
xcode&0.042&github&0.033&database&0.040\\
\hline
cocoa-touch&0.021&tfs&0.033&oracle&0.030\\
\hline
ipad&0.020&maven&0.029&sql-server-2008&0.029\\
\hline
cocoa&0.018&tortoisesvn&0.018&tsql&0.026\\
\hline
uitableview&0.012&msbuild&0.016&query&0.025\\
\hline
ios5&0.010&jenkins&0.015&sql-server-2005&0.019\\
\hline
core-data&0.009&tfs2010&0.014&database-design&0.011\\
\hline
\hline
\multicolumn{2}{|c|}{topic12} & \multicolumn{2}{c|}{topic13} & \multicolumn{2}{c|}{topic14}   \\
\hline
html&0.214&javascript&0.264&machine-learning&0.247\\
\hline
css&0.201&jquery&0.114&artificial-intelligence&0.130\\
\hline
xhtml&0.017&html&0.035&neural-network&0.062\\
\hline
web-development&0.016&ajax&0.031&classification&0.046\\
\hline
ie&0.012&css&0.016&data-mining&0.037\\
\hline
css-layout&0.010&firefox&0.013&svm&0.031\\
\hline
div&0.010&dom&0.011&weka&0.025\\
\hline
layout&0.010&php&0.011&libsvm&0.015\\
\hline
firefox&0.009&ie&0.010&nlp&0.024\\
\hline
ie6&0.009&web-development&0.008&bayesian&0.011\\
\hline
\end{tabular}
\end{table}

\section{Proposed approach: using DBpedia information}
\label{sec:chp6secSolution}

\subsection{Linking to DBpedia}

DBpedia\footnote{\url{http://dbpedia.org/about} (accessed Feb 2016)} is a crowd-sourced community effort to extract structured information from Wikipedia\footnote{\url{https://www.wikipedia.org/}(accessed Feb 2016)} and make this information available on the Web. It allows users to link their own dataset to Wikipedia data and to augment it with this huge amount of additional data, documents and links. The DBpedia knowledge base now plays an important role in enhancing the intelligence of Web applications and in supporting information integration. Among the advantages of the DBpedia knowledge base are the facts that it covers many domains and that it automatically evolves with Wikipedia changes. It currently describes 38.3 million things in total and contains 3 billions of RDF triples (2014 version).


In order to use the DBpedia knowledge base, a basic step is to link the bag of words to DBpedia. For example, the word \textit{javascript} could be linked to DBpedia resource \textit{\url{http://dbpedia.org/resource/JavaScript}}, the word \textit{beer} could be linked to DBpedia resource	\textit{\url{http://dbpedia.org/resource/Beer}}. However, in some cases, several resources or entities may correspond to the same word (homonymy). For instance, \textit{java} could be linked to the \textit{Java} island but it could also be linked to the\textit{Java} programming language. Therefore, we have to deal with a disambiguation problem when linking words to DBpedia resources. This is a well-known problem now and extensively studied by researchers working on entity recognition, named entity detection and entity linking.  
Babelfy \cite{chp6babelfy:Moroetal:14tacl} is a unified graph-based approach to solve Entity Linking (EL) and Word Sense Disambiguation (WSD) problems. Their experiments show the state-of-the-art performances on both tasks on 6 different datasets. Moreover they provide an online webservice\footnote{\url{http://babelfy.org} (accessed Feb 2016)}. So we directly used their web API to retrieval DBpedia links for the words in our dataset. In addition, we used classical similarity metrics to solve the disambiguation problem, as detailed in the next subsection.

\subsection{Using descriptions' cosine similarity for disambiguation}
Our main dataset is from the StackOverflow website and we found that there are detailed descriptions for each tag on the website, as shown in figure \ref{fig:chp6taginfo}
\begin{figure}[htp]
\centering
\includegraphics[width=5in]{chp6taginfo.jpg}  
\caption{Description of \textit{java} on StackOverflow dataset \footnote{\url{http://stackoverflow.com/tags/java/info} (accessed Feb 2016)}}
\label{fig:chp6taginfo} 
\end{figure}

Besides, each resource in DBpedia has a description.  We used the DBpedia keyword lookup service \footnote{\url{http://dbpedia.org/projects/dbpedia-lookup}(accessed Feb 2016)} to retrieve related resources for each tag. As Shown in Figure \ref{fig:chp6lookup}, the result of a call to the lookup service is a list of resources related to the given keyword.

\begin{figure}[htp]
\centering
\includegraphics[width=5in]{chp6lookup.jpg}  
\caption{Result of the DBpedia lookup service for keyword \textit{java}}
\label{fig:chp6lookup} 
\end{figure}

In order to link \textit{java} to the correct DBpedia resource, we compute the cosine similarity between the two descriptions from the two websites (StackOverflow and DBpedia) to solve the disambiguation problem. We show an example in Figure \ref{fig:chp6cosine}. The  entire procedure is described as following.

\begin{figure}[htp]
\centering
\includegraphics[width=5in]{chp6cosine.jpg}  
\caption{The example of disambiguation by computing the cosine distance of the descriptions }
\label{fig:chp6cosine} 
\end{figure}

%TODO Cath use a code  format like in the previous chapters for this algorithm
%done



\begin{algorithm}%[htp]
\begin{algorithmic}[1]
\label{algo:chp6link}
\State \textbf{Input:} \textit{ tag}
\State \textbf{Output:} \textit{tag-DB link}
\State \textit{//crawl the tag description from StackOverflow}
\State \textit{tagSO}=getTagDescriptionSO( \textit{tag} )
\State \textit{//retrieve DBpedia resources by the lookup service}
\State \textit{tagResouces}=getResourecesDB( \textit{tag} )
\State \textit{//compute the cosine distances between the description from StackOveflow and the description of each retrieved resource}
\State \textit{DBLink}=NULL
\State \textit{maxDis}=-1.0;
\For{ \textit{tagResouce} \textbf{in} \textit{tagResouces}}
\State \textit{dis}=consieDistance( \textit{tagSO}, \textit{tagResource.Descrition} )
\If{ ( \textit{dis} > \textit{maxDis} )}
\State \textit{//link the tag to the resource with the highest similarity}
\State \textit{DBlink}=\textit{tagResource.Link}
\State \textit{maxDis}=\textit{dis}
\EndIf

\EndFor
\State \textbf{return} \textit{tag},\textit{DBLink}

\end{algorithmic}
\end{algorithm}








% \begin{enumerate}
%     \item{crawl the tag description from StackOverflow}
%     \item{retrieve DBpedia resources by the lookup service}
%     \item{compute the cosine distances between the description from StackOveflow and the description of each retrieved resource}
%     \item{link the tag to the resource with the highest similarity}
% \end{enumerate}




\subsection{Creating graphs: retrieving potential links between resources}
After linking tags to theirs corresponding DBpedia resources, we then perform several SPARQL queries to retrieve the potential relations among the resources found for each topic. 


\begin{algorithm}%[htp]
\begin{algorithmic}[1]
\label{algo:dbpediaquery}
\State \textit{/***DBpedia graph extaction queries***/}
\Procedure{ExtractGraph}{ra,rb}
\State \textit{/***Depth=1:***/}
\State \textbf{select} \textit{?relation}
\State \textbf{where\{}
\State \ \ \ \ \ \textit{ra ?relation rb.}
\State \textbf{\}}

\State \textit{/***Depth=2:***/}
\State \textbf{select}  \textit{?r1, ?relation1, ?relation2} 
\State \textbf{where\{} 
\State \ \ \ \ \ \textit{ra ?relation1 ?r1.}
\State \ \ \ \ \ \textit{?r1 ?relation2 rb.} 
\State \textbf{\}} 

\State \textit{/***Depth=3:***/}
\State \textbf{select}  \textit{?r1, ?r2, ?relation1, ?relation2, ?relation3} 
\State \textbf{where\{} 
\State \ \ \ \ \ \textit{ra ?relation1 ?r1.}
\State \ \ \ \ \ \textit{?r1 ?relation2 ?r2.}
\State \ \ \ \ \ \textit{?r2 ?relation3 rb.} 
\State \textbf{\}} 
\end{algorithmic}
\end{algorithm}

where, \textit{ra}, \textit{rb} are the resources for which we want to retrieve the potential relations and $?r1$, $?r2$, $?relation1$, $?relation2$, $?relation3$ are the potential relations and the intermediate resources. $Depth$ denotes the hops between the resources \textit{ra} and \textit{rb}. We vary this parameter by $1,2,3$. Figure \ref{fig:chp6resourcegraph} shows the retrieved graph for the linux related topic.

To remain compatible with SPARQL 1.0 we did not use the path operator that would support a much synthetic way of writing these queries. The general idea behind these queries is to reconstruct a small connected graph around the detected resources for a topic in order to obtain a space where to analyze their relations.

\begin{figure}[htp]
\centering
\includegraphics[width=5in]{chp6resourcegraph.jpg}  
\caption{The example graph structure for the linux related topic}
\label{fig:chp6resourcegraph} 
\end{figure}

Once we have these relation graphs, we perform several graph algorithms to chose one or several resources as candidates to label the bag of words of the topics. We mainly used the following algorithms/metrics:
\begin{itemize}
    \item {InDegree (ID)} \\
    In a directed graph, for a node, the number of head ends adjacent to a node is called the indegree of the node. 
    \item {Betweenness Centrality (BC)} \\
    It is an indicator of a node's centrality in a network. It is equal to the number of shortest paths from all vertices to all others that pass through that node.
    \item {Degree Centrality (DC)} \\
    It is defined as the number of links incident upon a node, which is equal to indegree plus outdegree for a directed graph.
    \item {Page Rank \cite{chp6page1999pagerank} (PR)}\\
    PageRank is an algorithm used by Google Search to rank websites in their search engine results. It can be applied on other kinds of graph to estimate the importance of the nodes.
    \item {Random} \\
    We just randomly choose one node from the graph.
    \item {Top tags (Top)} \\
    The topic modeling algorithm generates a topic-word distribution to indicate to what extent a word is related to a topic. By sorting words' corresponding probabilities, we can obtain a ranked word list for each topic, which are the top related words in each topic. A naive approach can use the first tag or first two tags to label each topic. 
\end{itemize}

We proposed a method called "Most+Top" which consists in creating the list of the most recommended labels by all the above algorithms and then a label from this list by using the above \"Top tags\" algorithm. 


\section{Experiments: A survey study}
In order to evaluate the performances of the different ways to generate labels we conducted user studies on the results. We designed two surveys for the user study. Table \ref{tab:chp6survey} shows the structure of the survey we used. For each survey, we listed 30 topics, half of them are from StackOverflow dataset, half of them are from Flickr dataset. 
The only difference for survey A and B is the linking (disambiguation) method for StackOverflow dataset. As mentioned in section \ref{sec:chp6secSolution}, we use both cosine similarity and Babelfy to link tags with DBpedia resources. 


\begin{table}[htp]
\caption{Survey description and corresponding linking method}
\label{tab:chp6survey}
\centering
\begin{tabular}{|c|c|c|}
\hline
             &  15 StackOverflow Topics & 15 Flickr Topics \\ \hline
    Survey A & Cosine Similairy & Babelfy\\ \hline  
    Survey B &  Babelfy          & Babelfy\\ \hline
\end{tabular}

\end{table}


\subsection{Users' agreement}
We use Krippendorff's Alpha\footnote{\url{https://en.wikipedia.org/wiki/Krippendorff\'s_alpha}} score to evaluate the degree of agreement among users. The score indicates the homogeneity, or consensus, in the ratings given by users. The score is always smaller than 1, $\alpha = 1 $ indicates the judges reach a perfect agreement and $\alpha =0 $ indicates the judges do not agree at all. When $\alpha <0 $ this means that judges reached a disagreement exceeding what can be expected by chance.  
Figure \ref{fig:chp6alpha} illustrates the alpha score for 15 topics in each dataset and the average alpha score. We evaluate this score in three levels which correspond to the three sub figures. If we consider the top voted label as the best label, the first figure shows the agreement score among users. When we lower this limitation and consider the set of the top voted two labels as the best label, we can find that for most of the topics users could reach a good agreement. When we keep lowering this limitation and consider the top three voted labels as the best label, we can find that they reach an excellent agreement. 


\begin{figure}[htp]
\caption{Agreement Alpha Score on the top X labels}
\label{fig:chp6alpha} 
\centering
\includegraphics[width=5in]{chp6alpha.jpeg}  
\end{figure}

%TODO Cath Check the title (caption) of the figure

In addition, we calculate the proportion of top voted labels. Figure \ref{fig:chp6top} shows the number of topics which top voted labels take a certain proportion.
\begin{figure}[htp]
\centering
\includegraphics[width=5in]{chp6top.jpeg}  
\caption{Number of proportion as a function of the top X voted labels}
\label{fig:chp6top} 
\end{figure}
Proportion of top voted labels are plotted on the X-axis, and numbers of topics are plotted on the Y-axis. For instance, a data point ($50\%$,6) in the first sub figure means that there are 6 topics which first voted labels takes $50\%$ percent of all voted labels. A data point ($50\%$,6) in the second sub figure means that there are 6 topics which top two voted labels take $50\%$ percent of all voted labels. Similarly, a data point ($50\%$,6) in the third sub figure means that there are 6 topics which top three voted labels take $50\%$ percent of all voted labels. These three figures show that most of the labels chosen by judges are actually highly voted labels, which means all judges tend to agree on the top two or three labels.




\subsection{Quality evaluation: NDCG measurement}
We use the NDCG metric to evaluate all the algorithms listed in Section \ref{sec:chp6secSolution}. The Normalized DCG (NDCG) is introduced to compare different ranking lists. The value of NDCG is between 0.0 and 1.0. In our scenario, a NDCG@p value of 1.0 means detected interests and their order are totally the same as the labeled data until position \textit{p}, while a NDCG@p value of 0.0 means that the detected interests are completely different from the labeled data. Values between 0.0 to 1.0 mean that the detected interests are partially correct or ordered incorrectly. 
%It still 
Here, we evaluate NDCG@1, NDCG@2, and NDCG@3. The algorithm can generate a ranked label list. We sort the labels according to the number of votes from the survey as ideal ranking list. 
We can find that most of the algorithms can predict the good labels for a topic. Especially, if we consider giving two labels for a topic, our proposed method "Most+Top" has very good results on all the datasets, which means for all the topics, the method can generate two good labels to represent the meaning of the topic.

%TODO Cath a title (caption) is missing for this figure

\begin{figure}[htp]
\centering
\includegraphics[width=5in]{chp6ndcg.jpeg}  
\caption{NDCG score at position X}
\label{fig:chp6ndcg} 
\end{figure}




\section{Summary: representing a topic with labels}
In this chapter, we discussed how we used DBpedia as external knowledge to help choosing labels to turn bags of words into meaningful topics. From the user survey we found that users can reach a good agreement on composite labels. Therefore, it is more reasonable to have more than one keyword to label the bag of words of a topic. We also proposed a hybrid solution by combining results from different algorithms to generate composite labels to represent a topic. 

In the next chapter, we will focus on how to extract more sophisticated social information such as expertise, activity and trends.
